"""Checks concerning (usually renku) datasets"""

from renku.api import Activity, Project, Dataset
from renku.domain_model.project_context import project_context
from omnibenchmark.utils.user_input_checks import flatten
import requests
from omnibenchmark.utils.default_global_vars import DATA_QUERY_URL
from typing import Union, List, Mapping, Any
import os


def query_renku_api(
    url: str, page_num: int, page_item: int = 100
) -> List[Mapping[Any, Any]]:
    response = requests.get(url, params={"per_page": page_item, "page": page_num})
    return response.json()


def query_multipages(url: str, page_item: int = 100) -> List[Mapping[Any, Any]]:
    multi_page = True
    page_num = 1
    response: List = []
    while multi_page:
        res = query_renku_api(url, page_num=page_num, page_item=page_item)
        page_num += 1
        if len(res) < page_item:
            multi_page = False
        if isinstance(res, list):
            response.extend(res)
    return response


def renku_dataset_exist(slug: str, path: Union[os.PathLike, str] = os.getcwd()) -> bool:
    """Check if a renku dataset with a specific slug already exist at a certain project path.

    Args:
        slug (str): Dataset slug
        path (PathLike, optional): Project path to check. Defaults to ".".

    Returns:
        bool: True/False a dataset with that slug exist in the project at that path.
    """

    current_dir = os.getcwd()
    os.chdir(path)
    project_context.clear()
    #datasets = renku_api.renku_dataset_list()
    datasets = Dataset.list()
    matches = [dataset.slug for dataset in datasets if dataset.slug == slug]
    os.chdir(current_dir)
    return True if len(matches) >= 1 else False


def dataset_slug_exist(slug: str, data_query_url: str = DATA_QUERY_URL) -> bool:
    """Check if a renku dataset with a defined name already exists in the knowledge base.

    Args:
        slug (str): Slug to query
        data_query_url (str): Url of the knowledge base to query

    Returns:
        bool: True/False, if a dataset with that name already exist
    """

    url = data_query_url + slug
    response = query_multipages(url)
    # Checks to ensure no complete name matching (Remove if file matching is moved to triplet store queries)
    response_data = [res for res in response if res.get("type") == "dataset"]
    if slug in [item.get("slug") for item in response_data] or any(
        item.get("slug") in slug for item in response_data if item.get("slug") is not None  # type:ignore
    ):
        conflicting = [
            item.get("slug")
            for item in response_data
            if item.get("slug") is not None and slug in item.get("slug") or item.get("slug") is not None and item.get("slug") in slug # type:ignore
        ]
        nl = "\n"
        print(
            f"A dataset with a complete match of {slug} already exist.\n" # type:ignore
            f"Conflicting dataset name(s): {nl}{nl.join(conflicting)}.\n" # type:ignore
        )
        return True
    return True if slug in [item.get("slug") for item in response_data] else False


def find_activities_with_missing_inputs() -> List[Activity]:
    """Find activities related to missing input files

    Returns:
        List[Activity]: Activities that have missing input files linked
    """
    (
        outdated_outputs,
        outdated_activities,
        modified_inputs,
        deleted_inputs,
        modified_hidden_inputs
    ) = Project().status()
    return Activity.filter(inputs=list(deleted_inputs))


def find_outputs_with_missing_inputs() -> List[str]:
    """Find Outputs that were generated by activities with missing inputs

    Returns:
        List[str]: List of output files, that have missing input files
    """
    act_list = find_activities_with_missing_inputs()
    out_list = flatten([act.generated_outputs for act in act_list])
    return list(set([out.path for out in out_list]))
